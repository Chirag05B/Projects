{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW7OO3KRdLwGJJZim8VPLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chirag05B/Projects/blob/main/LangChain_PDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Talk to a pdf using LangChain and ChatGPT\n",
        "## Steps\n",
        "1. Loading the document\n",
        "2. Creating embeddings and Vectorization\n",
        "3. Querying the PDF"
      ],
      "metadata": {
        "id": "kTg8yHfa3Lbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loading the document"
      ],
      "metadata": {
        "id": "lePfbicb7TpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-sK1ltk3HJ1",
        "outputId": "7b418470-3d8a-4252-ccae-976613d0b7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 6745k  100 6745k    0     0  3042k      0  0:00:02  0:00:02 --:--:-- 3042k\n"
          ]
        }
      ],
      "source": [
        "# Download a paper using the curl command line tool directly from jupyter notebook\n",
        "!curl -o paper.pdf https://arxiv.org/pdf/2303.13519.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary dependencies of the project and import the modules we'll use for the project\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "from langchain.document_loaders import PyPDFLoader # For loading the pdf\n",
        "from langchain.embeddings import OpenAIEmbeddings # for creating embeddings\n",
        "from langchain.vectorstores import Chroma # for vectorization part\n",
        "from langchain.chains import ChatVectorDBChain # for chatting with the pdf\n",
        "from langchain.llms import OpenAI # the LLM model we'll use (ChatGPT)\n"
      ],
      "metadata": {
        "id": "Ju4cyPAx38XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = \"\" ## YOUR OPENAI API KEY ##"
      ],
      "metadata": {
        "id": "xskGUtT1CL66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load our pdf"
      ],
      "metadata": {
        "id": "lysCqwV96Neq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code uses the PyPDFLoader class from the langchain.document_loaders module\n",
        "# to load and split the PDF document into seperate pages or sections\n",
        "pdf_path = \"./paper.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load_and_split()\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDaonnIg5i-1",
        "outputId": "c8e5e9ad-c616-4192-ae48-06a49e6dd99f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning and Veriﬁcation of Task Structure in Instructional Videos\n",
            "Medhini Narasimhan1;2, Licheng Yu2, Sean Bell2, Ning Zhang2, Trevor Darrell1\n",
            "1UC Berkeley,2Meta AI\n",
            "https://medhini.github.io/task_structure\n",
            "Abstract\n",
            "Given the enormous number of instructional videos\n",
            "available online, learning a diverse array of multi-step task\n",
            "models from videos is an appealing goal. We introduce\n",
            "a new pre-trained video model, VideoTaskformer, focused\n",
            "on representing the semantics and structure of instructional\n",
            "videos. We pre-train VideoTaskformer using a simple and\n",
            "effective objective: predicting weakly supervised textual la-\n",
            "bels for steps that are randomly masked out from an instruc-\n",
            "tional video (masked step modeling). Compared to prior\n",
            "work which learns step representations locally, our ap-\n",
            "proach involves learning them globally, leveraging video of\n",
            "the entire surrounding task as context. From these learned\n",
            "representations, we can verify if an unseen video correctly\n",
            "executes a given task, as well as forecast which steps are\n",
            "likely to be taken after a given step. We introduce two new\n",
            "benchmarks for detecting mistakes in instructional videos,\n",
            "to verify if there is an anomalous step and if steps are exe-\n",
            "cuted in the right order. We also introduce a long-term fore-\n",
            "casting benchmark, where the goal is to predict long-range\n",
            "future steps from a given step. Our method outperforms pre-\n",
            "vious baselines on these tasks, and we believe the tasks will\n",
            "be a valuable way for the community to measure the quality\n",
            "of step representations. Additionally, we evaluate Video-\n",
            "Taskformer on 3 existing benchmarks—procedural activity\n",
            "recognition, step classiﬁcation, and step forecasting—and\n",
            "demonstrate on each that our method outperforms existing\n",
            "baselines and achieves new state-of-the-art performance.\n",
            "1. Introduction\n",
            "Picture this, you’re trying to build a bookshelf by watch-\n",
            "ing a YouTube video with several intricate steps. You’re\n",
            "annoyed by the need to repeatedly hit pause on the video\n",
            "and you’re unsure if you have gotten all the steps right so\n",
            "far. Fortunately, you have an interactive assistant that can\n",
            "guide you through the task at your own pace, verifying each\n",
            "\u0003Work done while an intern at Meta AI. Correspondence to\n",
            "medhini@berkeley.edu\n",
            "“Dip bread in batter”“Serve with maple syrup”\n",
            "EV1EV2EV3EV12\n",
            "Mask\n",
            "MaskVideoTaskformerT1T2T3T12Prediction over Step Classes\n",
            "“Dip bread in batter”EV1Prior work: Single clip step predictionOurs:Masked step prediction over all clips in video Figure 1: Prior work [13, 12] learns step representations from sin-\n",
            "gle short video clips, independent of the task, thus lacking knowl-\n",
            "edge of task structure. Our model, VideoTaskformer, learns step\n",
            "representations for masked video steps through the global context\n",
            "of all surrounding steps in the video, making our learned represen-\n",
            "tations aware of task semantics and structure.\n",
            "step as you perform it and interrupting you if you make a\n",
            "mistake. A composite task such as “ making a bookshelf ”\n",
            "involves multiple ﬁne-grained activities such as “ drilling\n",
            "holes ” and “ adding support blocks .” Accurately categoriz-\n",
            "ing these activities requires not only recognizing the indi-\n",
            "vidual steps that compose the task but also understanding\n",
            "the task structure, which includes the temporal ordering of\n",
            "the steps and multiple plausible ways of executing a step\n",
            "(e.g., one can beat eggs with a fork or a whisk). An ideal\n",
            "interactive assistant has both a high-level understanding of\n",
            "a broad range of tasks, as well as a low-level understanding\n",
            "of the intricate steps in the tasks, their temporal ordering,\n",
            "and the multiple ways of performing them.\n",
            "As seen in Fig. 1, prior work [12, 13] models step rep-\n",
            "resentations of a single step independent of the overall task\n",
            "context. This might not be the best strategy, given that steps\n",
            "for a task are related, and the way a step is situated in an\n",
            "overall task may contain important information about the\n",
            "step. To address this, we pre-train our model with a masked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Creating embeddings and Vectorization"
      ],
      "metadata": {
        "id": "4CzIbFWs7Kl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code creates embeddings using the OpenAIEbeddings class from the langchain.embeddings.openai\n",
        "# These embeddings are then passed to the Chroma class from the langchain.vectorstores module, which\n",
        "# generates a vector database for the given PDF document.\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=[OPENAI_API_KEY])\n",
        "vectordb = Chroma.from_documents(pages, embedding=embeddings, persist_directory = \".\")\n",
        "vectordb.persist()"
      ],
      "metadata": {
        "id": "hypyTREb6mQK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Querying"
      ],
      "metadata": {
        "id": "uXAoJG0Y-ewT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code sets up the ChatVectorDBChain class from langchain.chains to interact with ChatGPT using the previous generated vector database.\n",
        "# The query, provided as a command line argument is passed to the ChatVectorDBChain instance,\n",
        "# which returns an answer generated by ChatGPT\n",
        "\n",
        "pdf_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0.9, model_name='gpt-3.5-turbo',openai_api_key=[OPENAI_API_KEY]), vectordb, return_source_documents=True)\n",
        "\n",
        "query = 'What is the VideoTaskformer?'\n",
        "result = pdf_qa({'question': query, 'chat_history':''})\n",
        "print('Answer:')\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkoH2Tdm740X",
        "outputId": "0be81fab-7cf4-4992-b787-42cb23f9fd41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "VideoTaskformer is a pre-trained video model that focuses on representing the semantics and structure of instructional videos. It uses a simple and effective objective of predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling) to learn task-aware step representations from a large corpus of instructional videos. It can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. VideoTaskformer outperforms previous baselines on various benchmarks and achieves new state-of-the-art performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer:\n",
        "VideoTaskformer is a pre-trained video model that focuses on representing the semantics and structure of instructional videos. It uses a simple and effective objective of predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling) to learn task-aware step representations from a large corpus of instructional videos. It can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. VideoTaskformer outperforms previous baselines on various benchmarks and achieves new state-of-the-art performance.\n"
      ],
      "metadata": {
        "id": "geWdf3nfAvKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary and Final Thoughts\n",
        "The ability to ask questions and receive concise, relevant answers from a PDF document, can enable efficient engagement with the material, improving retention, understanding, and overall the entire learning experience.\n",
        "\n",
        "This example is just a glimpse of the immense possibilities that AI tools powered by LLMs can bring to the table."
      ],
      "metadata": {
        "id": "4WnnivfNBmN8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78UpVFAqBvsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}